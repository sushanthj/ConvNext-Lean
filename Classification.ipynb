{"cells":[{"cell_type":"markdown","metadata":{"id":"XBITN0M_LKds"},"source":["# HW2P2: Face Classification and Verification\n"]},{"cell_type":"markdown","metadata":{"id":"-NH4P-HzLRQs"},"source":["Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early!\n","\n","\n","*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7001 identities\n","*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities"]},{"cell_type":"markdown","metadata":{"id":"i1B_m84_cU6c"},"source":["Common errors which you may face in this homeworks (because of the size of the model)\n","\n","\n","*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BdoDIKWOMF59"},"source":["# Preliminaries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396,"status":"ok","timestamp":1696472556644,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"Jza7lwiScUhb","outputId":"0f7c7823-df0c-4ca6-ffd1-87ac061f361a"},"outputs":[],"source":["!nvidia-smi # to see what GPU you have"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTxfd_nqFnL9"},"outputs":[],"source":["!pip install wandb --quiet\n","!pip install ipdb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1696524146474,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"jwLEd0gdPbSc","outputId":"26de3001-ca75-401a-f28f-433c4438bd8d"},"outputs":[],"source":["import torch\n","import torch.nn.init as init\n","from torchsummary import summary\n","import torchvision #This library is used for image-based operations (Augmentations)\n","import os\n","import gc\n","from tqdm import tqdm\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","import glob\n","import wandb\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","import torchvision.transforms as transforms\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"1oxQNl-YVWHc"},"source":["# TODOs\n","As you go, please read the code and keep an eye out for TODOs!"]},{"cell_type":"markdown","metadata":{"id":"scOnMklwWBY6"},"source":["# Download Data from Kaggle"]},{"cell_type":"markdown","metadata":{"id":"mVODk7Yns2a5"},"source":["# Check the Dataset to see image shapes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMAedDFEp_Ha"},"outputs":[],"source":["DATA_DIR    = '/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/data/11-785-f23-hw2p2-classification/'# TODO: Path where you have downloaded the data\n","TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n","VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n","TEST_DIR    = os.path.join(DATA_DIR, \"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1696475706296,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"e5iGbjiup-vq","outputId":"f02cd696-db35-4f17-ec49-8e5618d05040"},"outputs":[],"source":["from torchvision.datasets import ImageFolder\n","\n","val_dataset = ImageFolder(VAL_DIR, transform=transforms.ToTensor())\n","\n","i = 0\n","for img, label in val_dataset:\n","    print(img.shape)\n","    print(label)\n","    i = i + 1\n","\n","    if i > 3:\n","      break"]},{"cell_type":"markdown","metadata":{"id":"Hbr7c1Q3tjqY"},"source":["## Find the Mean and Std Dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJWvIqMQtjYp"},"outputs":[],"source":["def get_mean_and_std(IMAGE_DATA_DIR):\n","    train_dataset = ImageFolder(IMAGE_DATA_DIR, transform=transforms.ToTensor())\n","\n","    # Initialize lists to store channel-wise means and standard deviations\n","    channel_wise_means = [0.0, 0.0, 0.0]\n","    channel_wise_stds = [0.0, 0.0, 0.0]\n","\n","    # Iterate through the training dataset to calculate means and standard deviations\n","    for image, _ in train_dataset:\n","        for i in range(3):  # Assuming RGB images\n","            channel_wise_means[i] += image[i, :, :].mean().item()\n","            channel_wise_stds[i] += image[i, :, :].std().item()\n","\n","    # Calculate the mean and standard deviation for each channel\n","    num_samples = len(train_dataset)\n","    channel_wise_means = [mean / num_samples for mean in channel_wise_means]\n","    channel_wise_stds = [std / num_samples for std in channel_wise_stds]\n","\n","    # Print the mean and standard deviation for each channel\n","    print(\"Mean:\", channel_wise_means)\n","    print(\"Std:\", channel_wise_stds)\n","\n","    return channel_wise_means, channel_wise_stds"]},{"cell_type":"markdown","metadata":{"id":"O68hT27SXClj"},"source":["# Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7qpMxG0XCJz"},"outputs":[],"source":["config = {\n","    'batch_size': 256, # Increase this if your GPU can handle it\n","    'lr': 4e-3,\n","    'epochs': 20, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n","    # Include other parameters as needed.\n","    'truncated_normal_mean' : 0,\n","    'truncated_normal_std' : 0.2,\n","}"]},{"cell_type":"markdown","metadata":{"id":"sSeiKHYrM-6b"},"source":["# Classification Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215421,"status":"ok","timestamp":1696527501559,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"tmRX5omaNDEZ","outputId":"6cd63cf4-ad36-487f-f1ae-76b5b76a1c9e"},"outputs":[],"source":["DATA_DIR    = '/content/data/11-785-f23-hw2p2-classification/'# TODO: Path where you have downloaded the data\n","TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n","VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n","TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n","\n","# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n","\n","# Means and standard dev found using disabled function above\n","channel_wise_means = [0.5102565238565329, 0.4014372720903177, 0.35085473649373455]\n","channel_wise_stds = [0.27081365674401414, 0.2362703534933915, 0.22260160982063917]\n","\n","train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.RandomHorizontalFlip(0.5),\n","    torchvision.transforms.ColorJitter(brightness=0.16, contrast=0.15, saturation=0.1),\n","    torchvision.transforms.RandomRotation(18),\n","    torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n","    torchvision.transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean=channel_wise_means, std=channel_wise_stds),\n","    torchvision.transforms.RandomErasing(p=0.3, scale=(0.05, 0.1)),\n","])# Implementing the right train transforms/augmentation methods is key to improving performance.\n","\n","# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n","# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n","# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n","\n","valid_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean=channel_wise_means, std=channel_wise_stds),\n","])\n","\n","\n","train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n","valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform=valid_transforms)\n","# You should NOT have data augmentation on the validation set. Why?\n","\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(\n","    dataset     = train_dataset,\n","    batch_size  = config['batch_size'],\n","    shuffle     = True,\n","    num_workers = 4,\n","    pin_memory  = True\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    dataset     = valid_dataset,\n","    batch_size  = config['batch_size'],\n","    shuffle     = False,\n","    num_workers = 2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqSR063BGE2e"},"outputs":[],"source":["# You can do this with ImageFolder as well, but it requires some tweaking\n","class ClassificationTestDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, transforms):\n","        self.data_dir   = data_dir\n","        self.transforms = transforms\n","\n","        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n","        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        return self.transforms(Image.open(self.img_paths[idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVLB41KtGC2o"},"outputs":[],"source":["test_dataset = ClassificationTestDataset(TEST_DIR, transforms = valid_transforms) #Why are we using val_transforms for Test Data?\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n","                         drop_last = False, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":601,"status":"ok","timestamp":1696527518170,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"x4t8eU9gY0Jy","outputId":"b2d8b04b-ad2f-4358-9a50-f0d4ec11f8a6"},"outputs":[],"source":["print(\"Number of classes    : \", len(train_dataset.classes))\n","print(\"No. of train images  : \", train_dataset.__len__())\n","print(\"Shape of image       : \", train_dataset[0][0].shape)\n","print(\"Batch size           : \", config['batch_size'])\n","print(\"Train batches        : \", train_loader.__len__())\n","print(\"Val batches          : \", valid_loader.__len__())"]},{"cell_type":"markdown","metadata":{"id":"zs2Xw_tl0IQ8"},"source":["## Data visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":10924,"status":"ok","timestamp":1696527531037,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"xIoRUzCbz85y","outputId":"b07a449c-7a97-4f31-e562-b06b316ef6cc"},"outputs":[],"source":["# Visualize a few images in the dataset\n","# You can write your own code, and you don't need to understand the code\n","# It is highly recommended that you visualize your data augmentation as sanity check\n","\n","# r, c    = [5, 5]\n","# fig, ax = plt.subplots(r, c, figsize= (15, 15))\n","\n","# k       = 0\n","# dtl     = torch.utils.data.DataLoader(\n","#     dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms\n","#     batch_size  = config['batch_size'],\n","#     shuffle     = True,\n","# )\n","\n","# for data in dtl:\n","#     x, y = data\n","\n","#     for i in range(r):\n","#         for j in range(c):\n","#             img = x[k].numpy().transpose(1, 2, 0)\n","#             ax[i, j].imshow(img)\n","#             ax[i, j].axis('off')\n","#             k+=1\n","#     break\n","\n","# del dtl"]},{"cell_type":"markdown","metadata":{"id":"mIqmojPaWD0H"},"source":["# Very Simple Network (for Mandatory Early Submission)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LayerNorm(torch.nn.Module):\n","    \"\"\"\n","    Using only channel_last method which is implemented in torch's layer_norm\n","    \"\"\"\n","    def __init__(self, normalized_shape, eps=1e-6, channels=\"first\"):\n","        super().__init__()\n","        self.weight = torch.nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))\n","        self.eps = eps\n","        self.normalized_shape = (normalized_shape, )\n","        self.channels = channels\n","\n","    def forward(self, x):\n","        if self.channels == \"last\":\n","            \"\"\"\n","            To use inbuilt layer_norm we permute from\n","            (batch_size, channels, height, width) -> (batch_size, height, width, channels)\n","            \"\"\"\n","            return torch.nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n","        elif self.channels == \"first\":\n","            ######### REIMPLEMENT THIS YOURSELF ######\n","            u = x.mean(1, keepdim=True)\n","            s = (x - u).pow(2).mean(1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.eps)\n","            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","            return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DropPath(torch.nn.Module):\n","    \"\"\"\n","    Stochastic Depth (we drop the non-shortcut path inside residual blocks with\n","                      some probability p)\n","    \"\"\"\n","\n","    def __init__(self, drop_probability = 0.0):\n","        super().__init__()\n","        self.drop_prob = drop_probability\n","\n","    def forward(self, x):\n","        # if drop prob is zero or in inference mode, skip this\n","        if np.isclose(self.drop_prob, 0.0, atol=1e-9) or not self.training:\n","          return x\n","\n","        # find output shape (eg. if input = 4D tensor, output = (1,1,1,1))\n","        # output_shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n","        output_shape = (x.shape[0],1,1,1)\n","\n","        # create mask of output shape and of input type on same device\n","        keep_mask = torch.empty(output_shape, dtype=x.dtype, device=DEVICE).bernoulli_((1-self.drop_prob))\n","        # Alternative: random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n","\n","        # NOTE: all methods like bernoulli_ with the underscore suffix means they\n","        # are inplace operations\n","        keep_mask.div_((1-self.drop_prob))\n","\n","        return x*keep_mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ConvNextBlock(torch.nn.Module):\n","    \"\"\"\n","    Refer : https://browse.arxiv.org/pdf/2201.03545v2.pdf for detailed architechture\n","\n","    \"\"\"\n","\n","    def __init__(self, num_ch, expansion_factor, drop_prob=0.0):\n","        # num_ch = number of channels at first and third layer of block\n","        # There'll be an expansion in the second layer given by expansion_factor\n","        super().__init__()\n","\n","        \"\"\"\n","        NOTE: To perform depthwise conv we use the param (groups=num_ch)\n","        to create a separate filter for each input channel\n","        \"\"\"\n","\n","\n","        self.main_block = torch.nn.Sequential(\n","            # 1st conv layer (deptwise)\n","            torch.nn.Conv2d(in_channels=num_ch, out_channels=num_ch,\n","                            kernel_size=7, padding=3, groups=num_ch),\n","            torch.nn.BatchNorm2d(num_ch),\n","\n","            # 2nd conv layer\n","            torch.nn.Conv2d(in_channels=num_ch, out_channels=num_ch*expansion_factor, kernel_size=1, stride=1), # 1x1 pointwise convs implemented as Linear Layer\n","            torch.nn.GELU(),\n","\n","            # 3rd conv layer\n","            torch.nn.Conv2d(in_channels=num_ch*expansion_factor, out_channels=num_ch, kernel_size=1, stride=1)\n","        )\n","\n","        for layer in self.main_block:\n","            if isinstance(layer, torch.nn.Conv2d):\n","                init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std'])\n","                init.constant_(layer.bias, 0)\n","\n","        # define the drop_path layer\n","        if drop_prob > 0.0:\n","            self.drop_residual_path = DropPath(drop_prob)\n","        else:\n","            self.drop_residual_path = torch.nn.Identity()\n","\n","    def forward(self, x):\n","        input = x.clone()\n","        x = self.main_block(x)\n","\n","        # sum the main and shortcut connection\n","        x = input + self.drop_residual_path(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":870,"status":"ok","timestamp":1696527691199,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"Ny-mh_ocWIJR","outputId":"6cbddad2-3a70-41cf-8229-89b474bc3e67"},"outputs":[],"source":["class Network(torch.nn.Module):\n","    \"\"\"\n","    ConvNext\n","    \"\"\"\n","\n","    def __init__(self, num_classes=7001, drop_rate=0.5, expand_factor=4):\n","        super().__init__()\n","\n","        self.backbone_out_channels = 400\n","        self.num_classes = num_classes\n","\n","        # number of channels at input/output of each res_blocks\n","        self.channel_list = [50, 175, 250, 400]\n","        # self.channel_list = [96, 192, 384, 768]\n","\n","        # number of repeats for each res_block\n","        self.block_repeat_list = [6,5,4,4]\n","        # self.block_repeat_list = [3,3,9,3]\n","\n","        # define number of stages from above\n","        self.num_stages = len(self.block_repeat_list)\n","\n","        self.drop_path_probabilities = [i.item() for i in torch.linspace(0, drop_rate, sum(self.channel_list))]\n","\n","        ############## DEFINE RES BLOCK AND AUX LAYERS ########################\n","\n","        # # Define the Stem (the first layer which takes input images)\n","        self.stem = torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels=3, out_channels=self.channel_list[0], kernel_size=4, stride=4),\n","            torch.nn.BatchNorm2d(self.channel_list[0]),\n","            )\n","\n","        # truncated normal initialization\n","        for layer in self.stem:\n","            if isinstance(layer, torch.nn.Conv2d):\n","                init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std'])\n","                init.constant_(layer.bias, 0)\n","\n","        # # Store the LayerNorm and Downsampling layer when switching btw 2 types of res_blocks\n","        # self.block_to_block_ln_and_downsample = []\n","        self.block_to_block_ln_and_downsample = [self.stem]\n","        for i in range(self.num_stages - 1):\n","            inter_downsample = torch.nn.Sequential(\n","                    torch.nn.BatchNorm2d(self.channel_list[i]),\n","                    torch.nn.Conv2d(in_channels=self.channel_list[i],\n","                                    out_channels=self.channel_list[i+1],\n","                                    kernel_size=2, stride=2)\n","                  )\n","            self.block_to_block_ln_and_downsample.append(inter_downsample)\n","\n","        # Store the Res_block stages (eg. 3xres_2, 3xres_3, ...)\n","        self.res_block_stages = torch.nn.ModuleList()\n","        for i in range(self.num_stages):\n","            res_block_layer = []\n","            for j in range(self.block_repeat_list[i]):\n","                res_block_layer.append(ConvNextBlock(num_ch=self.channel_list[i],\n","                                  expansion_factor=expand_factor,\n","                                  drop_prob=self.drop_path_probabilities[i+j]))\n","\n","            # append the repeated res_blocks as one layer\n","            # *res_block_layer means we add individual elements of the res_block_layer list\n","            self.res_block_stages.append(torch.nn.Sequential(*res_block_layer))\n","\n","        # truncated normal initialization\n","        for res_block_stage in self.res_block_stages:\n","            for layer in res_block_stage:\n","                if isinstance(layer, torch.nn.Conv2d):\n","                    init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std'])\n","                    init.constant_(layer.bias, 0)\n","\n","        # # final norm layer (here we use torch's own)\n","        # self.final_norm = torch.nn.LayerNorm(self.channel_list[-1], eps=1e-6)\n","\n","        # # final pool layer\n","        # self.final_pool = torch.nn.AdaptiveAvgPool2d((1,1))\n","\n","        #####################################################################\n","\n","        self.backbone = torch.nn.Sequential(\n","              # essentially stem (replace with stem if it works)\n","              self.block_to_block_ln_and_downsample[0],\n","              # res_1 block\n","              self.res_block_stages[0],\n","              self.block_to_block_ln_and_downsample[1],\n","              # res_2 block\n","              self.res_block_stages[1],\n","              self.block_to_block_ln_and_downsample[2],\n","              # res_3 block\n","              self.res_block_stages[2],\n","              self.block_to_block_ln_and_downsample[3],\n","              # res_4 block\n","              self.res_block_stages[3],\n","              torch.nn.AdaptiveAvgPool2d((1,1)),\n","              torch.nn.Flatten(),\n","            )\n","\n","        self.cls_layer = torch.nn.Sequential(\n","            torch.nn.Linear(self.backbone_out_channels, self.num_classes))\n","\n","        # truncated normal initialization\n","        for layer in self.cls_layer:\n","            if isinstance(layer, torch.nn.Linear):\n","                init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std'])\n","                init.constant_(layer.bias, 0)\n","\n","    def forward(self, x, return_feats=False):\n","        \"\"\"\n","        What is return_feats? It essentially returns the second-to-last-layer\n","        features of a given image. It's a \"feature encoding\" of the input image,\n","        and you can use it for the verification task. You would use the outputs\n","        of the final classification layer for the classification task.\n","\n","        You might also find that the classification outputs are sometimes better\n","        for verification too - try both.\n","        \"\"\"\n","        feats = self.backbone(x)\n","        out = self.cls_layer(feats)\n","\n","        if return_feats:\n","            return feats\n","        else:\n","            return out\n","\n","model = Network().to(DEVICE)\n","summary(model, (3, 224, 224))"]},{"cell_type":"markdown","metadata":{"id":"KZCn0qHuZRKj"},"source":["# Setup everything for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UowI9OcUYPjP"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # TODO: What loss do you need for a multi class classification problem?\n","optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], betas=(0.9, 0.999), weight_decay=0.05)\n","# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)  # T_max is the number of iterations in one cycle\n","\n","# TODO: Implement a scheduler (Optional but Highly Recommended)\n","# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n","\n","gamma = 0.6\n","milestones = [10,20,40,60,80]\n","\n","# scheduler1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.9, total_iters=5)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n","# scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[20, 51])\n","\n","scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n","# It is useful only in the case of compatible GPUs such as T4/V100"]},{"cell_type":"markdown","metadata":{"id":"dzM11HtcboYv"},"source":["# Let's train!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgSw6iJJavBZ"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, scheduler):\n","\n","    model.train()\n","\n","    # Progress Bar\n","    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    num_correct = 0\n","    total_loss  = 0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        optimizer.zero_grad() # Zero gradients\n","\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n","            outputs = model(images)\n","            loss    = criterion(outputs, labels)\n","\n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss      += float(loss.item())\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct = num_correct,\n","            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n","        )\n","\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update()\n","\n","        # TODO? Depending on your choice of scheduler,\n","        # You may want to call some schdulers inside the train function. What are these?\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss  = float(total_loss / len(dataloader))\n","\n","    scheduler.step()\n","\n","    return acc, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5V2UdnpdEoK"},"outputs":[],"source":["def validate(model, dataloader, criterion):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n","\n","    num_correct = 0.0\n","    total_loss = 0.0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss += float(loss.item())\n","\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct=num_correct)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss = float(total_loss / len(dataloader))\n","    return acc, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmotca6pcLLY"},"outputs":[],"source":["gc.collect() # These commands help you when you face CUDA OOM error\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"2mBgKGkXLrdJ"},"source":["# Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1696527791828,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"Ix62_BkaLr_D","outputId":"afff9e98-eee2-40e4-b0ba-e8f3feb613d3"},"outputs":[],"source":["wandb.login(key=\"\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"elapsed":11934,"status":"ok","timestamp":1696527816074,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"VG0vmsmbRYEi","outputId":"733671de-6c5b-4d04-b62c-071b9096fed6"},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"early-submission_lower_v2\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # run_id = ### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{"id":"SQkRw1FvLqYe"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKwqBB5LN4NV"},"outputs":[],"source":["# checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/Assignments/Assignment_2/checkpoints/low_cutoff_v2.pth'\n","checkpoint_path = '/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/checkpoints/Trial8.pth'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqWO8Edb0BK2","outputId":"bafa191b-65d1-4952-ed96-8ad48309858b"},"outputs":[],"source":["best_valacc = 0.0\n","\n","for epoch in range(config['epochs']):\n","\n","    curr_lr = float(optimizer.param_groups[0]['lr'])\n","\n","    train_acc, train_loss = train(model, train_loader, optimizer, criterion, scheduler)\n","\n","    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n","        epoch + 1,\n","        config['epochs'],\n","        train_acc,\n","        train_loss,\n","        curr_lr))\n","\n","    val_acc, val_loss = validate(model, valid_loader, criterion)\n","\n","    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n","\n","    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc,\n","               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n","\n","    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n","    # your learning rate differently\n","\n","    # #Save model in drive location if val_acc is better than best recorded val_acc\n","    if val_acc >= best_valacc:\n","      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n","      print(\"Saving model\")\n","      # save locally\n","      torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'scheduler_state_dict':scheduler.state_dict(),\n","                  'val_acc': val_acc,\n","                  'epoch': epoch}, './checkpoint.pth')\n","      # save in drive as well\n","      torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'scheduler_state_dict':scheduler.state_dict(),\n","                  'val_acc': val_acc,\n","                  'epoch': epoch}, checkpoint_path)\n","      best_valacc = val_acc\n","      # save in wandb\n","      wandb.save('checkpoint.pth')\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"UpgCHImRkYQW"},"source":["# Classification Task: Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2WQEUjXkWvo"},"outputs":[],"source":["def test(model,dataloader):\n","\n","  model.eval()\n","  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n","  test_results = []\n","\n","  for i, (images) in enumerate(dataloader):\n","      # TODO: Finish predicting on the test set.\n","      images = images.to(DEVICE)\n","\n","      with torch.inference_mode():\n","        outputs = model(images)\n","\n","      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n","      test_results.extend(outputs)\n","\n","      batch_bar.update()\n","\n","  batch_bar.close()\n","  return test_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69410,"status":"ok","timestamp":1696518527010,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"K7R1lcCAzULc","outputId":"80c23ebd-c2df-4362-84f4-2244f91809a8"},"outputs":[],"source":["test_results = test(model, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"zqfUzwS2L1gx"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vob9a2-HkW_V"},"outputs":[],"source":["with open(\"classification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(test_dataset)):\n","        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6523,"status":"ok","timestamp":1696518574365,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"GnRUN53CZMTf","outputId":"cf940eb6-793c-4fb6-84a7-0ae80ebe66be"},"outputs":[],"source":["!kaggle competitions submit -c 11-785-f23-hw2p2-classification -f classification_early_submission.csv -m \"early submission\""]},{"cell_type":"markdown","metadata":{"id":"7WYgUjJzUiGU"},"source":["# Verification Task: Validation"]},{"cell_type":"markdown","metadata":{"id":"FoBFFF8-Lpvj"},"source":["The verification task consists of the following generalized scenario:\n","- You are given X unknown identitites\n","- You are given Y known identitites\n","- Your goal is to match X unknown identities to Y known identities.\n","\n","We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.\n","\n","Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.\n","\n","This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification.\n","\n","## Download Verfication Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1696518625157,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"I6ipMFyIsoLe","outputId":"dc586f41-66de-4b14-bf46-92bf8c3428a2"},"outputs":[],"source":["!kaggle competitions download -c 11-785-f23-hw2p2-verification\n","!unzip -qo '11-785-f23-hw2p2-verification.zip' -d '/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data'"]},{"cell_type":"markdown","metadata":{"id":"1M9VTj72QGll"},"source":["## Load Weights used in Backbone and Classification Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1297,"status":"ok","timestamp":1696524637282,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"ahETJl2xQE6H","outputId":"20300d8d-e838-407b-bd58-c344445e7128"},"outputs":[],"source":["# Check if the checkpoint file exists\n","if os.path.exists(checkpoint_path):\n","    # If the checkpoint file exists, load the checkpoint\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n","    best_acc = checkpoint['val_acc']  # Update the best accuracy\n","    # Load the checkpoint and update the scheduler state if it exists in the checkpoint\n","    if 'scheduler_state_dict' in checkpoint:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        print(\"Loaded scheduler state from checkpoint.\")\n","    else:\n","        print(\"No scheduler state found in checkpoint.\")\n","    print(\"Loaded checkpoint from:\", checkpoint_path)\n","else:\n","    # If the checkpoint file does not exist, start training from scratch\n","    start_epoch = 0\n","    print(\"No checkpoint found at:\", checkpoint_path)"]},{"cell_type":"markdown","metadata":{"id":"KBInL1uGQXxP"},"source":["## Validation Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":763,"status":"ok","timestamp":1696524837297,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"f9aY5o-suWdn","outputId":"d2b9c49f-e75b-4920-8552-41d6167e2af5"},"outputs":[],"source":["# This obtains the list of known identities from the known folder\n","known_regex = \"/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/11-785-f23-hw2p2-verification/known/*/*\"\n","known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n","\n","# Obtain a list of images from unknown folders\n","unknown_dev_regex = \"/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/11-785-f23-hw2p2-verification/unknown_dev/*\"\n","unknown_test_regex = \"/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/11-785-f23-hw2p2-verification/unknown_test/*\"\n","\n","# We load the images from known and unknown folders\n","unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n","unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n","known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Why do you need only ToTensor() here?\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean=[0.5102565238565329, 0.4014372720903177, 0.35085473649373455],\n","                                     std=[0.27081365674401414, 0.2362703534933915, 0.22260160982063917])])\n","\n","unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n","unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n","known_images  = torch.stack([transforms(y) for y in known_images ])\n","#Print your shapes here to understand what we have done\n","\n","# You can use other similarity metrics like Euclidean Distance if you wish\n","similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuBAG3iA1dvG"},"outputs":[],"source":["print(unknown_dev_images[0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rk1LS0BRxFHM"},"outputs":[],"source":["def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):\n","\n","    # batch_size = 64\n","    unknown_feats, known_feats = [], []\n","\n","    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n","    model.eval()\n","\n","    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n","    for i in range(0, unknown_images.shape[0], batch_size):\n","        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n","        # print(\"unknown batch shape is\", unknown_batch.shape)\n","\n","        with torch.no_grad():\n","            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model\n","            # unknown_feat = unknown_feat.view(unknown_feat.size(0), -1) # NOTE: DOING THIS BECAUSE BACKBONE DOES NOT FLATTEN RN\n","            print(unknown_feat.shape)\n","        unknown_feats.append(unknown_feat)\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","\n","    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n","\n","    for i in range(0, known_images.shape[0], batch_size):\n","        known_batch = known_images[i:i+batch_size]\n","        # print(\"known batch shape is\", unknown_batch.shape)\n","        with torch.no_grad():\n","              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n","              # known_feat = known_feat.view(known_feat.size(0), -1) # NOTE: DOING THIS BECAUSE BACKBONE DOES NOT FLATTEN RN\n","              print(known_feat.shape)\n","\n","        known_feats.append(known_feat)\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","\n","    # Concatenate all the batches\n","    unknown_feats = torch.cat(unknown_feats, dim=0)\n","    known_feats = torch.cat(known_feats, dim=0)\n","\n","    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n","    # Print the inner list comprehension in a separate cell - what is really happening?\n","\n","    # max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n","    # max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n","\n","\n","    # Note that in unknown identities, there are identities without correspondence in known identities.\n","    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain\n","    # threshold compared with those identities with correspondence.\n","\n","    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n","    # pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n","\n","    # After early submission, remove the previous line and uncomment the following code\n","\n","    threshold = 0.5\n","    NO_CORRESPONDENCE_LABEL = 'n000000'\n","    pred_id_strings = []\n","    # for idx, prediction in enumerate(predictions):\n","    #     if max_similarity_values[idx] < threshold: # why < ? Thank about what is your similarity metric\n","    #         pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n","    #     else:\n","    #         pred_id_strings.append(known_paths[prediction])\n","\n","    # if mode == 'val':\n","    #   true_ids = pd.read_csv('/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/11-785-f23-hw2p2-verification/verification_dev.csv')['label'].tolist()\n","    #   accuracy = accuracy_score(pred_id_strings, true_ids)\n","    #   print(\"Verification Accuracy = {}\".format(accuracy))\n","\n","    return pred_id_strings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1696524855535,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"zMC7FacaUnJ7","outputId":"0bf7e67f-1200-4056-b76a-ab23117836d7"},"outputs":[],"source":["# verification eval\n","pred_id_strings = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], mode='val')\n","# verification test\n","pred_id_strings = eval_verification(unknown_test_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bCmzKfH7XgG"},"outputs":[],"source":["# add your finetune/retrain code here"]},{"cell_type":"markdown","metadata":{"id":"iTLW0RPD7XGC"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD-r-HmsAeWV"},"outputs":[],"source":["with open(\"/home/sush/CMU/11-785/11-785/Assignments/hw2/Part2/Full_Code/data/verification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(pred_id_strings)):\n","        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4862,"status":"ok","timestamp":1696474173951,"user":{"displayName":"Sushanth Jayanth","userId":"12955707965417826275"},"user_tz":240},"id":"jPIgq0tMZ8qk","outputId":"fa8f64a2-44f2-4120-a82c-8dad46a09b10"},"outputs":[],"source":["!kaggle competitions submit -c 11-785-f23-hw2p2-verification -f verification_early_submission.csv -m \"early submission\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlYxodjIvXxN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
